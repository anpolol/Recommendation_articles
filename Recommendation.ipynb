{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
    "#!pip install torch-geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/rusty1s/pyg_autoscale.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric_autoscale import metis, permute, SubgraphLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric_autoscale.models.base import ScalableGNN\n",
    "from torch_geometric_autoscale import metis, permute, SubgraphLoader\n",
    "from torch.nn import ModuleList\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import to_undirected\n",
    "import json\n",
    "from uuid import UUID\n",
    "from base64 import b64decode as b64d\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "def dataprocess(data): #удаление ребер для валидации \n",
    "        #splitting data to train and test\n",
    "        train_edge_index = []\n",
    "        val_edge_index = []\n",
    "        indices_to_delete_for_val  = np.random.choice(list(range(len(data.edge_index[0]))), int(len(data.edge_index[0])*0.2),replace=False)\n",
    "        l=0\n",
    "        for i,x in enumerate(list(zip(*data.edge_index.tolist()))):\n",
    "            l+=1\n",
    "            if i in indices_to_delete_for_val: \n",
    "                val_edge_index.append(x)\n",
    "            else: \n",
    "                train_edge_index.append(x)\n",
    "                \n",
    "        val_edge_index = torch.tensor(np.array(list(zip(*val_edge_index))))\n",
    "        train_edge_index = torch.tensor(np.array(list(zip(*train_edge_index))), dtype = torch.long)\n",
    "        \n",
    "        s = set(itertools.combinations(range(len(data.x)), 2))\n",
    "        \n",
    "        s_of_edges = set()\n",
    "        \n",
    "        for pair in (data.edge_index.t().tolist()):\n",
    "            s_of_edges.add(tuple(pair))\n",
    "        s_of_non_edges = s - s_of_edges\n",
    "        \n",
    "        #append negative samples to test set\n",
    "        non_edges=[]\n",
    "        for pair in list(s_of_non_edges):\n",
    "            non_edges.append(list(pair))\n",
    "        non_edges_val=torch.tensor(random.choices(non_edges, k = len(val_edge_index[0]))).t()\n",
    "        y_true_val = [1]*len(val_edge_index[0])\n",
    "        val_edge_index=torch.cat((val_edge_index,non_edges_val),1)\n",
    "        y_true_val += [0]*len(non_edges_val[0])\n",
    "        data.edge_index = train_edge_index\n",
    "        \n",
    "        return data,non_edges_val,val_edge_index,y_true_val #data уже обрезанная по ребрам\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(): #загрузить и обработать данные\n",
    "    with open('assets (9)','r', encoding=\"utf-8\") as f:\n",
    "        assets = json.load(f)\n",
    "\n",
    "    with open('asset_embeddings (1)','r') as f:\n",
    "        embeddings = json.load(f)\n",
    "\n",
    "    to_uuid = lambda x: str(UUID(b64d(x, altchars=None, validate=False).hex()))\n",
    "    \n",
    "    rels = []\n",
    "    \n",
    "    for asset in assets:\n",
    "        if asset['cited_articles']:\n",
    "            source = to_uuid(asset['asset_id']['$binary'])\n",
    "            for tt in  asset['cited_articles']:\n",
    "                target = to_uuid(tt['$binary'])\n",
    "                rels.append(tuple([source,target]))\n",
    "    emb_map = {}\n",
    "   \n",
    "    for emb in embeddings:\n",
    "        emb_map[to_uuid(emb['asset_id']['$binary'])] = emb['embedding']\n",
    "\n",
    "    rels = [x for x in rels if x[0] in emb_map and x[1] in emb_map]\n",
    "    mapping_old_to_new={}\n",
    "    mapping_new_to_old = {}\n",
    "    \n",
    "    for id_new,id_old in enumerate(emb_map):\n",
    "            mapping_old_to_new[id_old] = id_new\n",
    "            mapping_new_to_old[id_new] = id_old\n",
    "            \n",
    "    x = []\n",
    "    for id_new,id_old in enumerate(emb_map):\n",
    "        if id_new!=len(emb_map)-1:\n",
    "            x.append(emb_map[id_old]) \n",
    "        \n",
    "    edges = list(map(lambda edge: [mapping_old_to_new[edge[0]], mapping_old_to_new[edge[1]]], rels)) \n",
    "\n",
    "    edges_to_graph = list(filter(lambda edge: (edge[0] != len(emb_map)-1) and ((edge[1] != len(emb_map)-1)) , edges)) \n",
    "    edges_to_vertex = list(filter(lambda edge: (edge[0] == len(emb_map)-1) or ((edge[1] == len(emb_map)-1)) , edges)) \n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges_to_graph) #чтоб граф точно был направленным, в Graph ребра в одну сторону выписываются, хотя и считаются ненаправленными\n",
    "    \n",
    "    data_init = Data(x=torch.tensor(x),edge_index = torch.tensor(list(G.edges)).t() ) #тут все ребра\n",
    "    return  data_init,mapping_new_to_old,mapping_old_to_new,edges_to_vertex, emb_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def di_to_undirected(data):\n",
    "    data.edge_index=to_undirected(data.edge_index)\n",
    "    data.adj_t=SparseTensor(row=data.edge_index[0],col=data.edge_index[1],sparse_sizes=(len(data.x),len(data.x)))\n",
    "    #SparseTensor.from_edge_index(data.edge_index)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_init, mapping_new_to_old,mapping_old_to_new,edges_to_vertex,embs=data_load() #data_init - это самый изначальный граф, маппинги вершин\n",
    "data,non_edges_val,val_edge_index,y_true_val = dataprocess(data_init) #data - здесь уже без части ребер, для валидации надо\n",
    "data = di_to_undirected(data)#добавление ребер в обратную сторону\n",
    "data_init = di_to_undirected(data_init)#добавление ребер в обратную сторону\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05296272353579912,\n",
       " 0.9175472138192473,\n",
       " 0.8811217302271236,\n",
       " 0.2920485259320418,\n",
       " 0.10029090411773556,\n",
       " 0.3676452328179284,\n",
       " 0.8947273944380368,\n",
       " 0.5764944820296138,\n",
       " 0.7326820953580538,\n",
       " 0.042754954421536207,\n",
       " 0.3706365740548918,\n",
       " 0.6710640699294622,\n",
       " 0.2652880363984014,\n",
       " 0.9769814198410949,\n",
       " 0.7853958298319698,\n",
       " 0.9484088995882706,\n",
       " 0.6842910280364134,\n",
       " 0.7817298027711445,\n",
       " 0.7309043207813105,\n",
       " 0.21625588423551645,\n",
       " 0.40192949319020876,\n",
       " 0.8090304001925255,\n",
       " 0.6704646355686849,\n",
       " 0.65978472052646,\n",
       " 0.21293798506251294,\n",
       " 0.13561407291207084,\n",
       " 0.5463965096491107,\n",
       " 0.12263575082207556,\n",
       " 0.07798522033507171,\n",
       " 0.9114051680911228,\n",
       " 0.7906265011528475,\n",
       " 0.24613787839846002,\n",
       " 0.23476188571274414,\n",
       " 0.9445137939824617,\n",
       " 0.014288409442186345,\n",
       " 0.23364235633147634,\n",
       " 0.877874043483939,\n",
       " 0.1941345116521237,\n",
       " 0.5640604222155272,\n",
       " 0.48304610429646344,\n",
       " 0.63054154603766,\n",
       " 0.33917760870318747,\n",
       " 0.8701075392083494,\n",
       " 0.8400646215865778,\n",
       " 0.015752271807421447,\n",
       " 0.7717771883685767,\n",
       " 0.11517887062171883,\n",
       " 0.24558768236917372,\n",
       " 0.9532430798300097,\n",
       " 0.09783100476276463,\n",
       " 0.5356851816174091,\n",
       " 0.6721938818169279,\n",
       " 0.26901392347525677,\n",
       " 0.5760212308665867,\n",
       " 0.941396730758282,\n",
       " 0.7433681814954896,\n",
       " 0.8188607741679697,\n",
       " 0.4787466661349796,\n",
       " 0.9438698348405481,\n",
       " 0.582650272823021,\n",
       " 0.20318826620079322,\n",
       " 0.5361776278038571,\n",
       " 0.3368199348401306,\n",
       " 0.5045961008717775,\n",
       " 0.06275071129651533,\n",
       " 0.6883456466386806,\n",
       " 0.6629852115048982,\n",
       " 0.19225449198853262,\n",
       " 0.7935217970123902,\n",
       " 0.9942629369260825,\n",
       " 0.24689177768811632,\n",
       " 0.9263503536786298,\n",
       " 0.1401577358138496,\n",
       " 0.4864340505430459,\n",
       " 0.4112735694356078,\n",
       " 0.6774114543067429,\n",
       " 0.25993010900773283,\n",
       " 0.20673632353561255,\n",
       " 0.7017252314795405,\n",
       " 0.46482812953081853,\n",
       " 0.3373605464309051,\n",
       " 0.42576217515514125,\n",
       " 0.5838839786133438,\n",
       " 0.9607298475555681,\n",
       " 0.48326927248927753,\n",
       " 0.23833995793380036,\n",
       " 0.6687161118238525,\n",
       " 0.6265677435485012,\n",
       " 0.5038926771867662,\n",
       " 0.16884025276368864,\n",
       " 0.7631787889701716,\n",
       " 0.4515368593171085,\n",
       " 0.2639615897617047,\n",
       " 0.46372866032092097,\n",
       " 0.2644227477029222,\n",
       " 0.3862701339747173,\n",
       " 0.0947272609518659,\n",
       " 0.5831673115134136,\n",
       " 0.3501668511149846,\n",
       " 0.6648142775888831,\n",
       " 0.7614398644457484,\n",
       " 0.7948737323372678,\n",
       " 0.3251561112184296,\n",
       " 0.3218285104098235,\n",
       " 0.8369807621649259,\n",
       " 0.9404215651784317,\n",
       " 0.11908143240718738,\n",
       " 0.7963789730369241,\n",
       " 0.7520384006210621,\n",
       " 0.44566312731639546,\n",
       " 0.1087831775757565,\n",
       " 0.11378698480478344,\n",
       " 0.06032367277480244,\n",
       " 0.939781292730059,\n",
       " 0.8912301377428676,\n",
       " 0.3873982839271435,\n",
       " 0.4224872817793971,\n",
       " 0.4986613605694592,\n",
       " 0.9802032857880537,\n",
       " 0.39504028614791875,\n",
       " 0.0367966134833122,\n",
       " 0.045570356132394574,\n",
       " 0.3005908462622531,\n",
       " 0.8592183646523109,\n",
       " 0.8285683397067396,\n",
       " 0.09226303714413009,\n",
       " 0.9451121491144934,\n",
       " 0.8533555194484612,\n",
       " 0.6541895157682421,\n",
       " 0.8541608509693899,\n",
       " 0.25220209674426963,\n",
       " 0.23629840604977626,\n",
       " 0.9264039951857023,\n",
       " 0.5848944395155186,\n",
       " 0.5614191734474144,\n",
       " 0.10936195289243178,\n",
       " 0.6542598144605912,\n",
       " 0.8270380648238591,\n",
       " 0.2568504778911598,\n",
       " 0.2435671955949048,\n",
       " 0.9062560806746359,\n",
       " 0.4357915653594413,\n",
       " 0.7404212195669462,\n",
       " 0.37874180238150523,\n",
       " 0.17463222819300162,\n",
       " 0.19635948314749418,\n",
       " 0.6302825157665756,\n",
       " 0.25081489836477444,\n",
       " 0.73875751083981,\n",
       " 0.6300582547219199,\n",
       " 0.6588358889869451,\n",
       " 0.5057712656409452,\n",
       " 0.3405972296497184,\n",
       " 0.17273095501919777,\n",
       " 0.5811968909351475,\n",
       " 0.5393611999483531,\n",
       " 0.4525535899645704,\n",
       " 0.9093326999816977,\n",
       " 0.5065659408546209,\n",
       " 0.6461870894505941,\n",
       " 0.18205687853300512,\n",
       " 0.5295398119171947,\n",
       " 0.3988199243523587,\n",
       " 0.6421447342445937,\n",
       " 0.12079242932374523,\n",
       " 0.17455811548281686,\n",
       " 0.6998254827209895,\n",
       " 0.4846568070577669,\n",
       " 0.37164553227331965,\n",
       " 0.4712924151118869,\n",
       " 0.2784511628719265,\n",
       " 0.15941659048831247,\n",
       " 0.4393188552579421,\n",
       " 0.8983398914042253,\n",
       " 0.5441159636757172,\n",
       " 0.13069409047627767,\n",
       " 0.17748835531919416,\n",
       " 0.6618488183665898,\n",
       " 0.48179513763471105,\n",
       " 0.03249338758266196,\n",
       " 0.45593516316401717,\n",
       " 0.21184253583468304,\n",
       " 0.7699354727612295,\n",
       " 0.7490637254185172,\n",
       " 0.8134330876831647,\n",
       " 0.4733563860113619,\n",
       " 0.08861068026716767,\n",
       " 0.1619875921828392,\n",
       " 0.2932737686248593,\n",
       " 0.3229710164943397,\n",
       " 0.1277283262685004,\n",
       " 0.5771352603407741,\n",
       " 0.21868867436630546,\n",
       " 0.20085639329061722,\n",
       " 0.4404878853943086,\n",
       " 0.8321250850729718,\n",
       " 0.671152910548484,\n",
       " 0.0009013662859541638,\n",
       " 0.8817191709816486,\n",
       " 0.03703800859417006,\n",
       " 0.44540634997591255,\n",
       " 0.0055180781228734155,\n",
       " 0.8855293790465162,\n",
       " 0.794256607013295,\n",
       " 0.2271657343479927,\n",
       " 0.38146809576731244,\n",
       " 0.43335447332030874,\n",
       " 0.529035723589616,\n",
       " 0.8204503715132421,\n",
       " 0.1077234838943868,\n",
       " 0.8208672622324835,\n",
       " 0.46822079870540345,\n",
       " 0.27509377329622675,\n",
       " 0.370788639825004,\n",
       " 0.8879815788566813,\n",
       " 0.24655369335544197,\n",
       " 0.32194907765027425,\n",
       " 0.4366091062419888,\n",
       " 0.22106160996283508,\n",
       " 0.7772775744550784,\n",
       " 0.6088006919395768,\n",
       " 0.17757784849025526,\n",
       " 0.378252608098094,\n",
       " 0.2861783402542787,\n",
       " 0.1119702850420482,\n",
       " 0.32598849444146627,\n",
       " 0.9543312130399797,\n",
       " 0.2028566655306946,\n",
       " 0.950702333206834,\n",
       " 0.7578397408321883,\n",
       " 0.19315905172060244,\n",
       " 0.8522488966493907,\n",
       " 0.8848622298517655,\n",
       " 0.18124294982033018,\n",
       " 0.35393616677881823,\n",
       " 0.4919372040671792,\n",
       " 0.014314544547199715,\n",
       " 0.027515258263039466,\n",
       " 0.9068371115625318,\n",
       " 0.652472552017018,\n",
       " 0.8185152846949985,\n",
       " 0.06327831696829234,\n",
       " 0.42641911779242925,\n",
       " 0.23100011990673963,\n",
       " 0.5468969498911741,\n",
       " 0.04962517391780075,\n",
       " 0.15893060814248194,\n",
       " 0.5392235484742037,\n",
       " 0.35974316831557274,\n",
       " 0.22151350375222634,\n",
       " 0.5590833675722993,\n",
       " 0.5092126582145569,\n",
       " 0.37024023675627704,\n",
       " 0.08782806184225556,\n",
       " 0.8791088977368771,\n",
       " 0.8810288136840503,\n",
       " 0.7724407170981727,\n",
       " 0.2905315209890035,\n",
       " 0.1764842892413072,\n",
       " 0.3105191558284597,\n",
       " 0.864702661326422,\n",
       " 0.20716461442396839,\n",
       " 0.04783981140543714,\n",
       " 0.14690564141288842,\n",
       " 0.5913348510175114,\n",
       " 0.02637927639453319,\n",
       " 0.14396025717858718,\n",
       " 0.5463349243952561,\n",
       " 0.27439822449392504,\n",
       " 0.07878522091812779,\n",
       " 0.6865836984029826,\n",
       " 0.41548349930088646,\n",
       " 0.373456402151082,\n",
       " 0.03165605719621234,\n",
       " 0.41923086871581416,\n",
       " 0.944574161387986,\n",
       " 0.335942320239796,\n",
       " 0.8818794782799674,\n",
       " 0.4372336019228602,\n",
       " 0.4837110159928314,\n",
       " 0.6746861775640354,\n",
       " 0.7931356155500987,\n",
       " 0.6749327606166413,\n",
       " 0.5778334741648211,\n",
       " 0.345311284916232,\n",
       " 0.20312743233949448,\n",
       " 0.9639337427178044,\n",
       " 0.5564803808056775,\n",
       " 0.9885639795218084,\n",
       " 0.5619541759033848,\n",
       " 0.19109022732194214,\n",
       " 0.8717890632459857,\n",
       " 0.6835537468964661,\n",
       " 0.252510738044486,\n",
       " 0.356975872390779,\n",
       " 0.26207821838282486,\n",
       " 0.6265506660792723,\n",
       " 0.7761040767543625,\n",
       " 0.724321300039524,\n",
       " 0.8547466952685044,\n",
       " 0.2888331574111648,\n",
       " 0.7105432473591696,\n",
       " 0.2421310385106823,\n",
       " 0.05006573432058947,\n",
       " 0.3897858127933157,\n",
       " 0.26762152021813235,\n",
       " 0.04302017011889692,\n",
       " 0.6722637338070222,\n",
       " 0.9604345940285617,\n",
       " 0.1353986678793011,\n",
       " 0.7046448108114829,\n",
       " 0.260387171301524,\n",
       " 0.9659425309958837,\n",
       " 0.9221696873276111,\n",
       " 0.8254279340930979,\n",
       " 0.16934196157861792,\n",
       " 0.4704635338224882,\n",
       " 0.3790287496297401,\n",
       " 0.29350629426485975,\n",
       " 0.061527433782146645,\n",
       " 0.012545947116749612,\n",
       " 0.0008554560550677603,\n",
       " 0.5397783852054647,\n",
       " 0.12763662305693668,\n",
       " 0.6378179919620294,\n",
       " 0.3994893654241707,\n",
       " 0.9578811234577811,\n",
       " 0.6745732483936328,\n",
       " 0.9184541201805386,\n",
       " 0.21846308606514908,\n",
       " 0.9471676677962823,\n",
       " 0.7009602914408766,\n",
       " 0.1126045037903799,\n",
       " 0.38186782490284443,\n",
       " 0.5470143802315941,\n",
       " 0.308410444342719,\n",
       " 0.44007299954589973,\n",
       " 0.270113837884693,\n",
       " 0.06882543860900914,\n",
       " 0.9922062306245921,\n",
       " 0.5517101170970504,\n",
       " 0.5774881190234877,\n",
       " 0.6969874560818637,\n",
       " 0.3195958027001836,\n",
       " 0.4090391080472521,\n",
       " 0.23707160039363828,\n",
       " 0.5598858339596954,\n",
       " 0.8257023468640095,\n",
       " 0.463688010937071,\n",
       " 0.8304593753044407,\n",
       " 0.4466661445951263,\n",
       " 0.07511886462903661,\n",
       " 0.2994530779209259,\n",
       " 0.045481896653641773,\n",
       " 0.03928118296506711,\n",
       " 0.011660948023497486,\n",
       " 0.016888151913701743,\n",
       " 0.9001781192056376,\n",
       " 0.500606548539565,\n",
       " 0.08585428568080677,\n",
       " 0.5114704756481757,\n",
       " 0.606033955426234,\n",
       " 0.3426475050506933,\n",
       " 0.1972617846600231,\n",
       " 0.5396198121591359,\n",
       " 0.6617459238447814,\n",
       " 0.34449838167505986,\n",
       " 0.315092996758357,\n",
       " 0.9756476656902604,\n",
       " 0.202258157336915,\n",
       " 0.3379466743167777,\n",
       " 0.5012658010372248,\n",
       " 0.5445651206042739,\n",
       " 0.7108319498422246,\n",
       " 0.26644927255855555,\n",
       " 0.48465178061270386,\n",
       " 0.9323018836721159,\n",
       " 0.2425915980637704,\n",
       " 0.07614403362600408,\n",
       " 0.3167846336181821,\n",
       " 0.0004941153063997561,\n",
       " 0.5531488182305905,\n",
       " 0.3343122645176373,\n",
       " 0.07280001032998962,\n",
       " 0.16090119960318872,\n",
       " 0.5714746511373731,\n",
       " 0.3238144065868045,\n",
       " 0.6203987727544297,\n",
       " 0.44809924383046607,\n",
       " 0.24094786095747756,\n",
       " 0.48156942356791,\n",
       " 0.45430037690088376,\n",
       " 0.12443458886880443,\n",
       " 0.9838114286384897,\n",
       " 0.2633026437297906,\n",
       " 0.5502190881084394,\n",
       " 0.8227721037965793,\n",
       " 0.22009232331107198,\n",
       " 0.6035031976145644,\n",
       " 0.7472669288246151,\n",
       " 0.8841630748727806,\n",
       " 0.9728786218392074,\n",
       " 0.7927885902044761,\n",
       " 0.48315287580920885,\n",
       " 0.562322985047425,\n",
       " 0.49594607880655206,\n",
       " 0.47201996573968685,\n",
       " 0.19192785919285382,\n",
       " 0.3924419107662821,\n",
       " 0.8110676083426843,\n",
       " 0.5085978635241526,\n",
       " 0.5766687804023619,\n",
       " 0.05760301721489858,\n",
       " 0.20563302249587567,\n",
       " 0.14964513279646685,\n",
       " 0.1709618552386306,\n",
       " 0.8312319598169152,\n",
       " 0.8654634444158883,\n",
       " 0.6624551545495889,\n",
       " 0.10224678392188158,\n",
       " 0.1314857531886161,\n",
       " 0.14899535002466457,\n",
       " 0.44953288010044257,\n",
       " 0.11777117363487755,\n",
       " 0.035729514485071445,\n",
       " 0.8668906114500411,\n",
       " 0.3490900485239481,\n",
       " 0.15118265131055142,\n",
       " 0.02082989680367553,\n",
       " 0.23833279658956574,\n",
       " 0.965098445697641,\n",
       " 0.7873035071693405,\n",
       " 0.535099116965185,\n",
       " 0.4621064002744618,\n",
       " 0.8090189218940061,\n",
       " 0.681378636247003,\n",
       " 0.8580294776063376,\n",
       " 0.08519464958788348,\n",
       " 0.966214885768234,\n",
       " 0.49375409238544943,\n",
       " 0.863886295578189,\n",
       " 0.7316323315549753,\n",
       " 0.22159591844708415,\n",
       " 0.4463693329369135,\n",
       " 0.8831163971855647,\n",
       " 0.7465105536789771,\n",
       " 0.16251959875239164,\n",
       " 0.5169425645793028,\n",
       " 0.7955411455750929,\n",
       " 0.8597494592116164,\n",
       " 0.8975825089126145,\n",
       " 0.8991191830965273,\n",
       " 0.4064152116089911,\n",
       " 0.9118067460167797,\n",
       " 0.9715319342647422,\n",
       " 0.4763937786171917,\n",
       " 0.9989186795695609,\n",
       " 0.34349916055945984,\n",
       " 0.9628672484790682,\n",
       " 0.5422337599611093,\n",
       " 0.10316532931065503,\n",
       " 0.9456236756070805,\n",
       " 0.2657786417634743,\n",
       " 0.10665073549068815,\n",
       " 0.9458392595118611,\n",
       " 0.6808622466901755,\n",
       " 0.5672796929333034,\n",
       " 0.10604205603729733,\n",
       " 0.09429390007646643,\n",
       " 0.5990315141692681,\n",
       " 0.8726883570974533,\n",
       " 0.048404812955188725,\n",
       " 0.25396709536544104,\n",
       " 0.5171906615023965,\n",
       " 0.08473247596755173,\n",
       " 0.027237085742646938,\n",
       " 0.06847961841432937,\n",
       " 0.6669933814832624,\n",
       " 0.522202604719717,\n",
       " 0.9490012756750531,\n",
       " 0.6386456395333177,\n",
       " 0.7244940837405691,\n",
       " 0.27862517071195403,\n",
       " 0.6634847812235531,\n",
       " 0.17308423509784232,\n",
       " 0.2541997665213783,\n",
       " 0.711878115914758,\n",
       " 0.4903138058635188,\n",
       " 0.9437523040596276,\n",
       " 0.2249459855383512,\n",
       " 0.16308635926612836,\n",
       " 0.9606942920597195,\n",
       " 0.9356732715699243,\n",
       " 0.02325128138220911,\n",
       " 0.9895460866934411,\n",
       " 0.45698891461917457,\n",
       " 0.9761238622773103,\n",
       " 0.8526176273861922,\n",
       " 0.21397712810241454,\n",
       " 0.44212447377174924,\n",
       " 0.7706500721861469,\n",
       " 0.3597613126766671,\n",
       " 0.6319114263748659,\n",
       " 0.059681399084093756,\n",
       " 0.8964525372724459,\n",
       " 0.12245025418863487,\n",
       " 0.1331853863254504,\n",
       " 0.1690856332447931,\n",
       " 0.4663342329950386,\n",
       " 0.8327675524201318,\n",
       " 0.5521888611485226,\n",
       " 0.8973616896813871,\n",
       " 0.16046427502938265,\n",
       " 0.5022451409832694,\n",
       " 0.5358837680146896,\n",
       " 0.7766408087644503,\n",
       " 0.7008400052573904,\n",
       " 0.18265620992607856,\n",
       " 0.743416783666193,\n",
       " 0.6910767066188669,\n",
       " 0.7721200694577184,\n",
       " 0.15766810561790978,\n",
       " 0.01313643254112773,\n",
       " 0.4187822133006951,\n",
       " 0.2793516348663978,\n",
       " 0.27412904210136513,\n",
       " 0.4773847500891961,\n",
       " 0.5046788473652031,\n",
       " 0.8556027252407085,\n",
       " 0.8882633588764077,\n",
       " 0.7976147654727342,\n",
       " 0.9598185021206438,\n",
       " 0.030016012293900496,\n",
       " 0.7208733035944037,\n",
       " 0.9671580138991358,\n",
       " 0.2815194173944431,\n",
       " 0.3983188511267599,\n",
       " 0.8617333774949953,\n",
       " 0.1566432819887893,\n",
       " 0.5075068928230985,\n",
       " 0.039566508101381404,\n",
       " 0.9771190494878736,\n",
       " 0.9773778630841944,\n",
       " 0.3192168125198791,\n",
       " 0.5065141571516084,\n",
       " 0.1637323239501579,\n",
       " 0.010000000558855193,\n",
       " 0.09654564283049227,\n",
       " 0.7508518512366458,\n",
       " 0.4711052273603574,\n",
       " 0.9565417342690766,\n",
       " 0.12678111444810525,\n",
       " 0.7916848607950981,\n",
       " 0.15095229345486627,\n",
       " 0.9944850078884212,\n",
       " 0.7820879874610992,\n",
       " 0.2997531699300935,\n",
       " 0.3641431023693785,\n",
       " 0.885143853009094,\n",
       " 0.5694465455149429,\n",
       " 0.5402557039540549,\n",
       " 0.5928062186237865,\n",
       " 0.3719517858000213,\n",
       " 0.28607048376064115,\n",
       " 0.5772096987589098,\n",
       " 0.5482530652130695,\n",
       " 0.06398715039658331,\n",
       " 0.09293331714427511,\n",
       " 0.6533541627251164,\n",
       " 0.7404173790628463,\n",
       " 0.9458851401092424,\n",
       " 0.0726298022129731,\n",
       " 0.7652898067868087,\n",
       " 0.817342902272117,\n",
       " 0.8983554131502408,\n",
       " 0.46942687823575546,\n",
       " 0.7200513913955501,\n",
       " 0.6920481633394888,\n",
       " 0.4907450679939773,\n",
       " 0.7609324181965105,\n",
       " 0.6381954789905467,\n",
       " 0.2238531163552071,\n",
       " 0.8694597849411977,\n",
       " 0.05512570367492953,\n",
       " 0.435274827358371,\n",
       " 0.29675841078375986,\n",
       " 0.33968801413325944,\n",
       " 0.11380731478579265,\n",
       " 0.06028534301715249,\n",
       " 0.20721859215143823,\n",
       " 0.08736158886380896,\n",
       " 0.3373799140057484,\n",
       " 0.035223484572772135,\n",
       " 0.20043273098931846,\n",
       " 0.6271421913350274,\n",
       " 0.6983126414021291,\n",
       " 0.7083263506716126,\n",
       " 0.8651443628377451,\n",
       " 0.8460899703119836,\n",
       " 0.42603352662277016,\n",
       " 0.7716499658980089,\n",
       " 0.3337852911537773,\n",
       " 0.3761458450038646,\n",
       " 0.5181273999168189,\n",
       " 0.5859198697962933,\n",
       " 0.44627022022752216,\n",
       " 0.59268844420185,\n",
       " 0.5962963101614759,\n",
       " 0.5747647456617295,\n",
       " 0.12347158670116232,\n",
       " 0.0061645754116286255,\n",
       " 0.9415658124786732,\n",
       " 0.9983011867780339,\n",
       " 0.9450614014077419,\n",
       " 0.9573966647400555,\n",
       " 0.5817346164375838,\n",
       " 0.4977066599500758,\n",
       " 0.5989667008542948,\n",
       " 0.16710508263093826,\n",
       " 0.0678960941444976,\n",
       " 0.9325061208588445,\n",
       " 0.4179064227975168,\n",
       " 0.41618461592855605,\n",
       " 0.9421365550970802,\n",
       " 0.6645715994012104,\n",
       " 0.5378296274791854,\n",
       " 0.4037962411915432,\n",
       " 0.6211887019765768,\n",
       " 0.6908323384059707,\n",
       " 0.8570988813955804,\n",
       " 0.6071207687500172,\n",
       " 0.9875396149331765,\n",
       " 0.9448662360654615,\n",
       " 0.9067249814868424,\n",
       " 0.07510014475610038,\n",
       " 0.40180021847608804,\n",
       " 0.3751458782523909,\n",
       " 0.6856554483830303,\n",
       " 0.5952249750370305,\n",
       " 0.07300403577126047,\n",
       " 0.15053584579838264,\n",
       " 0.7092689611838453,\n",
       " 0.6116396527133693,\n",
       " 0.2688265329245416,\n",
       " 0.3488662058359401,\n",
       " 0.045020014692117694,\n",
       " 0.44245114698441934,\n",
       " 0.9501029275129902,\n",
       " 0.16117016376235016,\n",
       " 0.6942448131539971,\n",
       " 0.9686725145356206,\n",
       " 0.09974223632280477,\n",
       " 0.9516971259157597,\n",
       " 0.3361547855460435,\n",
       " 0.7312731684357534,\n",
       " 0.06937079531391344,\n",
       " 0.06741838315151516,\n",
       " 0.28814811350134695,\n",
       " 0.7362569969951499,\n",
       " 0.23603393583750498,\n",
       " 0.31281630666704585,\n",
       " 0.6039664429434217,\n",
       " 0.1864017780539684,\n",
       " 0.133429312658788,\n",
       " 0.4018558468831668,\n",
       " 0.6802076646773448,\n",
       " 0.4423093026825684,\n",
       " 0.5105620519098722,\n",
       " 0.3737745022306648,\n",
       " 0.40955748358348343,\n",
       " 0.7030527808966429,\n",
       " 0.30697223383367334,\n",
       " 0.7453705511513198,\n",
       " 0.348122605589989,\n",
       " 0.9445375655430205,\n",
       " 0.9694801605924812,\n",
       " 0.9227073113535281,\n",
       " 0.402648987085756,\n",
       " 0.917986907629478,\n",
       " 0.48645711274695524,\n",
       " 0.29675750370837894,\n",
       " 0.1855693720594236,\n",
       " 0.6727280742018921,\n",
       " 0.7128140470299252,\n",
       " 0.9443372104474747,\n",
       " 0.8825685037104986,\n",
       " 0.4637903828569795,\n",
       " 0.7387806419505542,\n",
       " 0.37191115115199047,\n",
       " 0.8623412648420984,\n",
       " 0.08017065389156275,\n",
       " 0.37400410029751574,\n",
       " 0.3450401198961396,\n",
       " 0.9502507996123147,\n",
       " 0.5729923599347401,\n",
       " 0.29780836270660294,\n",
       " 0.34572448068435024,\n",
       " 0.5614209491616442,\n",
       " 0.04535279649279622,\n",
       " 0.26675482697792097,\n",
       " 0.4112211190765239,\n",
       " 0.2594546859223502,\n",
       " 0.5566229172196488,\n",
       " 0.9828766106238829,\n",
       " 0.8217830709242144,\n",
       " 0.025617756776315947,\n",
       " 0.8971393690408598,\n",
       " 0.35202448483940674,\n",
       " 0.145339320675979,\n",
       " 0.23278531830660387,\n",
       " 0.3876109650425289,\n",
       " 0.5735491625049687,\n",
       " 0.06321282167550202,\n",
       " 0.36775090590670245,\n",
       " 0.2543994213264015,\n",
       " 0.13459095825493927,\n",
       " 0.7398837508920519,\n",
       " 0.12376801715334895,\n",
       " 0.7353324707987318,\n",
       " 0.9859092384176943,\n",
       " 0.6017220688328819,\n",
       " 0.8995074530569918,\n",
       " 0.9705558395558314,\n",
       " 0.09444434011312142,\n",
       " 0.8698580550820498,\n",
       " 0.03452667844810009,\n",
       " 0.6315336014608985,\n",
       " 0.5693835627421712,\n",
       " 0.9403227491319628,\n",
       " 0.7793523797459302,\n",
       " 0.16523163844983524,\n",
       " 0.2738136699549355,\n",
       " 0.6897462711768895,\n",
       " 0.7734159021904373,\n",
       " 0.41714857745882894,\n",
       " 0.44876680446239814,\n",
       " 0.2178320788721626,\n",
       " 0.35998094176863393,\n",
       " 0.9659550044267267,\n",
       " 0.7639160962744983,\n",
       " 0.09608633002584965,\n",
       " 0.3015666654545045,\n",
       " 0.9360307199017737,\n",
       " 0.5211594609009903,\n",
       " 0.1729594132512483,\n",
       " 0.03933747734572668,\n",
       " 0.2521534847500201,\n",
       " 0.38772706147067226,\n",
       " 0.768655827980656,\n",
       " 0.38841033858804497,\n",
       " 0.07392490745818303,\n",
       " 0.7542168570722387,\n",
       " 0.00790121102362007,\n",
       " 0.3002075082049245,\n",
       " 0.8689823872529062,\n",
       " 0.6101447770603887,\n",
       " 0.4198222115186363,\n",
       " 0.7168152282066685,\n",
       " 0.5844456981269042,\n",
       " 0.5509599891585535,\n",
       " 0.8497360853298431,\n",
       " 0.45102953703421134,\n",
       " 0.01603515176975856,\n",
       " 0.8709870881210907,\n",
       " 0.015551150675541692,\n",
       " 0.29450246539611113,\n",
       " 0.28672874344215304,\n",
       " 0.3795761562466353]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[]\n",
    "for i in range(768):\n",
    "    l.append((random.random()))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex = (mapping_new_to_old[1282],embs[mapping_new_to_old[1282]])\n",
    "neighbors = [(mapping_new_to_old[587],embs[mapping_new_to_old[587]]),(mapping_new_to_old[589],embs[mapping_new_to_old[589]]), ('any',l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "neighbors_of_neighbors=[]\n",
    "\n",
    "for neig in [587,589]:\n",
    "        for neig_of_neig in (data_init.edge_index[0]==torch.tensor(neig)).nonzero()[0]:\n",
    "            neighbors_of_neighbors.append((mapping_new_to_old[int(data_init.edge_index[1][neig_of_neig])],embs[mapping_new_to_old[int(data_init.edge_index[1][neig_of_neig])]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_of_neighbors=[]\n",
    "\n",
    "for neig in [587,589]:\n",
    "        for neig_of_neig in (data_init.edge_index[1]==torch.tensor(neig)).nonzero()[0]:\n",
    "            neighbors_of_neighbors.append((mapping_new_to_old[int(data_init.edge_index[0][neig_of_neig])],embs[mapping_new_to_old[int(data_init.edge_index[0][neig_of_neig])]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning with 40 parts... Done! [0.01s]\n",
      "Permuting data... Done! [0.01s]\n"
     ]
    }
   ],
   "source": [
    "perm, ptr = metis(data.adj_t, num_parts=40, log=True)\n",
    "data_train = permute(data, perm, log=True)\n",
    "loader = SubgraphLoader(data_train, ptr, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalable(ScalableGNN):\n",
    "    def __init__(self, data, num_nodes, in_channels, hidden_channels,\n",
    "                 out_channels, num_layers,mapping_new_to_old,mapping_old_to_new):\n",
    "        # * pool_size determines the number of pinned CPU buffers\n",
    "        # * buffer_size determines the size of pinned CPU buffers,\n",
    "        #   i.e. the maximum number of out-of-mini-batch nodes\n",
    "\n",
    "        super().__init__(num_nodes, hidden_channels, num_layers,\n",
    "                         pool_size=2, buffer_size=5000)\n",
    "        self.out_channels=out_channels\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "        self.mapping_new_to_old = mapping_new_to_old\n",
    "        self.data = data\n",
    "        self.mapping_old_to_new=mapping_old_to_new\n",
    "    def forward(self, x, adj_t, *args):\n",
    "        for i, (conv, history) in enumerate(zip(self.convs, self.histories)):\n",
    "            if i == len(self.convs)-1:\n",
    "                x = conv(x, adj_t)\n",
    "            else:\n",
    "                x = conv(x, adj_t).relu_()\n",
    "            x = self.push_and_pull(history, x, *args)\n",
    "            \n",
    "        return x # self.convs[-1](x, adj_t)\n",
    "    def inference(self,vertex,neighbors,neighbors_of_neighbors,*args):\n",
    "         #предполагаю что среди neighbors_of_neighbors- нет новых\n",
    "            #нет проверки на то, пришли ко мне одинаковые вершины или разные\n",
    "        model.eval()\n",
    "        #обработка новых вершин\n",
    "        new_embs=1\n",
    "        if vertex[0] not in self.mapping_old_to_new:  \n",
    "                self.mapping_old_to_new[vertex[0]] = len(self.mapping_old_to_new)\n",
    "                self.mapping_new_to_old[self.mapping_old_to_new[vertex[0]]] = vertex[0]\n",
    "                self.data.x = torch.cat((data.x,torch.tensor(vertex[1]).reshape(1,-1)))\n",
    "        for vid in list(zip(*neighbors))[0]:\n",
    "            if vid not in self.mapping_old_to_new:  \n",
    "                self.mapping_old_to_new[vid] = len(self.mapping_old_to_new)\n",
    "                self.mapping_new_to_old[self.mapping_old_to_new[vid]] = vid\n",
    "                self.data.x = torch.cat((data.x,torch.tensor(vertex[1]).reshape(1,-1)))\n",
    "                new_embs+=1\n",
    "        \n",
    "        new_edges = []\n",
    "        new_ver = self.mapping_old_to_new[vertex[0]]\n",
    "        for neighbor in neighbors:\n",
    "            new_edges.append([new_ver,self.mapping_old_to_new[neighbor[0]]])\n",
    "            new_edges.append([self.mapping_old_to_new[neighbor[0]],new_ver])\n",
    "        self.data.edge_index =  torch.cat([data.edge_index,torch.tensor(new_edges).t()],-1)\n",
    "        \n",
    "        #обновляем сам граф\n",
    "        x = self.data.x\n",
    "        self.data.adj_t = SparseTensor.from_edge_index(data.edge_index)#SparseTensor(row=self.data.edge_index[0],col=self.data.edge_index[1],sparse_sizes=(len(self.data.x),len(self.data.x)))\n",
    "        #adj_t = self.data.adj_t\n",
    "        \n",
    "        #делаем батч только нужных нам вершин и соседей\n",
    "        c1=list(zip(*neighbors))[0]\n",
    "        c2=list(zip(*neighbors))[1]\n",
    "        c1+=(list(zip(*neighbors_of_neighbors))[0])\n",
    "        c2+=(list(zip(*neighbors_of_neighbors))[1])\n",
    "        all_neighbors=list(zip(c1,c2)) #(old index, input embedding)\n",
    "        \n",
    "        remap_new_to_new2 = {}\n",
    "        remap_new2_to_new = {}\n",
    "        remap_new_to_new2[0] = self.mapping_old_to_new[vertex[0]]\n",
    "        remap_new_to_new2[self.mapping_old_to_new[vertex[0]]] = 0\n",
    "        for new_index, neigbor in enumerate(c1):\n",
    "            remap_new_to_new2[self.mapping_old_to_new[neigbor]] = new_index+1\n",
    "            remap_new2_to_new[new_index+1] = self.mapping_old_to_new[neigbor]\n",
    "        \n",
    "        #находим ребра\n",
    "        \n",
    "        first = ((self.data.edge_index[0]==self.mapping_old_to_new[vertex[0]]).nonzero(as_tuple=True)[0]).tolist()\n",
    "        second =  ((self.data.edge_index[1]==self.mapping_old_to_new[vertex[0]]).nonzero(as_tuple=True)[0]).tolist()\n",
    "        \n",
    "        for neighbor in c1: \n",
    "            first+=(((self.data.edge_index[0]==self.mapping_old_to_new[neighbor]).nonzero(as_tuple=True)[0]).tolist())\n",
    "            second+=(((self.data.edge_index[1]==self.mapping_old_to_new[neighbor]).nonzero(as_tuple=True)[0]).tolist())\n",
    "        \n",
    "        \n",
    "        edge_index_indices = list(set(first).intersection(set(second)))\n",
    "        edge_index_1 = self.data.edge_index[0][torch.tensor(edge_index_indices)]\n",
    "        edge_index_1=list(map( lambda x: remap_new_to_new2[x], edge_index_1.tolist()))\n",
    "        edge_index_2 = self.data.edge_index[1][torch.tensor(edge_index_indices)]\n",
    "        edge_index_2=list(map( lambda x: remap_new_to_new2[x], edge_index_2.tolist()))\n",
    "        batch_edge_index = torch.stack((torch.tensor(edge_index_1),torch.tensor(edge_index_2))) \n",
    "        \n",
    "        #делаем из него adj_t#перепроверить возможно не правильно так делать \n",
    "        adj_t=SparseTensor.from_edge_index(batch_edge_index)\n",
    "        #собираем х!!!! \n",
    "        x=[vertex[1]]\n",
    "        x+=c2\n",
    "        \n",
    "        x=torch.tensor(x)\n",
    "        \n",
    "        #сам inference\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if i == len(self.convs)-1:\n",
    "                x = conv(x, adj_t)\n",
    "            else:\n",
    "                x = conv(x, adj_t).relu_()\n",
    "        \n",
    "        out=[]\n",
    "        for i in range(new_embs,0,-1):\n",
    "            new_index = len(self.mapping_old_to_new)-i\n",
    "            out.append((self.mapping_new_to_old[new_index],x[remap_new_to_new2[new_index]]))\n",
    "        return out \n",
    "    \n",
    "    def loss(self,out, PosNegSamples):\n",
    "        (pos_rw,neg_rw) = PosNegSamples\n",
    "            # Negative loss\n",
    "        start, rest = neg_rw[:, 0].type(torch.LongTensor), neg_rw[:, 1:].type(torch.LongTensor).contiguous()\n",
    "        h_start =out[start].view(neg_rw.size(0), 1,self.out_channels)\n",
    "        \n",
    "        h_rest =  out[rest.view(-1)].view(neg_rw.size(0), -1,self.out_channels)\n",
    "        dot = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        neg_loss = -torch.log(torch.sigmoid((-1)*dot)).mean()\n",
    "            # Positive loss.\n",
    "        start, rest = pos_rw[:, 0].type(torch.LongTensor), pos_rw[:, 1].type(torch.LongTensor).contiguous()\n",
    "        h_start = out[start].view(pos_rw.size(0), 1,self.out_channels)\n",
    "\n",
    "        h_rest = out[rest.view(-1)].view(pos_rw.size(0), -1,self.out_channels)\n",
    "        dot = ((h_start * h_rest).sum(dim=-1)).view(-1)\n",
    "        pos_loss = -(torch.log(torch.sigmoid(dot))).mean() \n",
    "        return pos_loss + neg_loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_samples(edge_index,ind,num_negative_samples): #возвращает негативные примеры\n",
    "    d=datetime.now()\n",
    "    neg_samples=torch.Tensor(size=(len(ind),num_negative_samples+1))\n",
    "    neg_samples.t()[0] = ind\n",
    "    for i in ind:\n",
    "        d=datetime.now()\n",
    "        neig=(edge_index[1][(edge_index[0]==i).nonzero(as_tuple=True)[0]])\n",
    "        neg_neigbors=ind[~ind.unsqueeze(1).eq(neig).any(1)] #difference between all indices and neighbors\n",
    "        neg_neigbors=(neg_neigbors[(neg_neigbors!=i).nonzero(as_tuple=True)[0]])\n",
    "        #probs = torch.zeros(max(neg_neigbors)+1)\n",
    "        d=datetime.now()\n",
    "        #probs[neg_neigbors]=1\n",
    "        d = datetime.now()\n",
    "        neg_samples[i][1:]=torch.tensor(np.random.choice(neg_neigbors,num_negative_samples))\n",
    "        \n",
    "    \n",
    "    return neg_samples\n",
    "\n",
    "def edge_index(batch,size): #выбираются ребра только для батча и только для вершин внутри батча\n",
    "    adj=batch.adj_t\n",
    "    t=(adj.to_torch_sparse_coo_tensor()).coalesce()\n",
    "    ind = torch.tensor(range(size))\n",
    "    edge_index = t.indices()\n",
    "    first=((edge_index[0]<len(ind)).nonzero(as_tuple=True)[0])\n",
    "    second=((edge_index[1]<len(ind)).nonzero(as_tuple=True)[0])\n",
    "    new_indices=(torch.tensor(np.intersect1d(first,second)))\n",
    "    edge_index1 =  edge_index[0][new_indices]\n",
    "    edge_index2 = edge_index[1][new_indices]\n",
    "    return (torch.stack([edge_index1, edge_index2], dim=0)),ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,loader,num_negative_smples):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch,*args in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x[:args[0]], batch.adj_t,*args)\n",
    "        e_index,ind=edge_index(batch,args[0])\n",
    "        #print('edge index construct',datetime.now()-d)\n",
    "        pos = e_index.t()\n",
    "        \n",
    "        neg = neg_samples(e_index,ind,num_negative_smples)\n",
    "        samples = (pos,neg)\n",
    "        loss = model.loss(out, samples)\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        total_loss+=loss\n",
    "    return total_loss/len(loader)\n",
    "def test(y_true,model,data,val_edge_index):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.adj_t)\n",
    "\n",
    "    y_pred = []\n",
    "    for x in list(zip(*val_edge_index)):\n",
    "        y_pred.append(float(torch.sigmoid(torch.dot(out[x[0]],out[x[1]]))))#print(torch.sigmoid(torch.dot(out[x[0]],out[x[1]])))\n",
    "    \n",
    "    return roc_auc_score(y_true,torch.tensor(y_pred).detach().numpy()) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1282"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_init.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1282"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_index=mapping_new_to_old[len(embs)-1]\n",
    "mapping_new_to_old.pop(len(embs)-1)\n",
    "mapping_old_to_new.pop(old_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1282"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping_old_to_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.4276, grad_fn=<DivBackward0>) 0.7404883654122378\n",
      "1 tensor(1.3401, grad_fn=<DivBackward0>) 0.7824992818155703\n",
      "2 tensor(1.2623, grad_fn=<DivBackward0>) 0.8060787130135019\n",
      "3 tensor(1.2003, grad_fn=<DivBackward0>) 0.8167193335248493\n",
      "4 tensor(1.1799, grad_fn=<DivBackward0>) 0.8213272048261994\n",
      "5 tensor(1.1530, grad_fn=<DivBackward0>) 0.8295547256535478\n",
      "6 tensor(1.1773, grad_fn=<DivBackward0>) 0.8415053145647804\n",
      "7 tensor(1.1307, grad_fn=<DivBackward0>) 0.8508014938236139\n",
      "8 tensor(1.1218, grad_fn=<DivBackward0>) 0.8593507612754955\n",
      "9 tensor(1.1020, grad_fn=<DivBackward0>) 0.8638437230680839\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.390705\n"
     ]
    }
   ],
   "source": [
    "d = datetime.now()\n",
    "model = Scalable(data_init, num_nodes=len(data.x), in_channels=len(data.x[0]), hidden_channels=128,out_channels=128, num_layers=2, mapping_new_to_old=mapping_new_to_old,mapping_old_to_new=mapping_old_to_new)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay = 1e-5)\n",
    "num_negative_smples=4\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=0.01, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "losses=[]\n",
    "scores=[]\n",
    "\n",
    "for epoch in range(10):\n",
    "        d=datetime.now()\n",
    "        loss=train(model,loader,num_negative_smples)\n",
    "        losses.append(float(loss))\n",
    "        scheduler.step(loss)\n",
    "        score=test(y_true_val,model,data,val_edge_index)\n",
    "        scores.append(score)\n",
    "        print(epoch,loss,score)\n",
    "plt.plot(torch.tensor(losses).tolist())\n",
    "plt.show()\n",
    "plt.plot(torch.tensor(scores).tolist())\n",
    "plt.show()\n",
    "print(datetime.now()-d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сам инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "x=model.inference(vertex,neighbors,neighbors_of_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = igraph.Graph.TupleList(rels, weights=False, directed=True)\n",
    "igraph.summary(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bf2009fb-5859-499d-9fe2-7334bb23ad9d',\n",
       "  tensor([-3.0107e-02,  9.4633e-02,  4.5891e-02, -6.6886e-02, -8.7278e-02,\n",
       "           1.3694e-01, -1.4957e-01, -4.3116e-02,  5.9198e-02, -9.7398e-02,\n",
       "           2.0575e-03,  9.2069e-02, -7.6678e-02,  9.2932e-02, -5.5742e-03,\n",
       "          -2.4954e-01, -6.8726e-02, -1.3190e-02,  1.8677e-02,  4.8087e-02,\n",
       "          -1.5204e-01, -9.5113e-02,  3.7323e-02,  7.0868e-02,  1.9982e-02,\n",
       "           8.9551e-02,  1.0408e-02,  7.5286e-02, -5.1205e-02, -1.3756e-01,\n",
       "           3.4742e-02, -4.6605e-02,  4.2456e-03,  2.9065e-02, -2.5053e-01,\n",
       "           6.2558e-04,  1.2581e-01,  8.3188e-02, -8.0341e-02, -6.2990e-02,\n",
       "          -1.6177e-02, -9.9823e-04, -3.7491e-02, -8.3898e-02,  1.7528e-01,\n",
       "           4.3861e-02, -2.5226e-01, -1.0818e-02, -3.1307e-02, -2.4857e-02,\n",
       "           1.6376e-01,  1.3063e-01, -2.1551e-02,  1.7644e-02, -1.6836e-01,\n",
       "          -1.2791e-02, -4.2412e-03, -2.8947e-02, -9.5810e-03,  2.2692e-03,\n",
       "          -9.3626e-02,  1.4776e-01, -2.0646e-02,  9.8707e-02,  1.2780e-01,\n",
       "          -7.9568e-02, -9.4566e-02,  1.9657e-01, -3.8693e-02,  1.1038e-01,\n",
       "          -4.9183e-02, -9.9355e-02, -1.1374e-01, -4.0428e-02, -7.5750e-02,\n",
       "          -2.5007e-02,  1.1145e-01, -1.6614e-01,  2.5036e-01,  1.0372e-01,\n",
       "           6.7116e-02,  1.2876e-02,  1.0198e-01,  6.7444e-02,  1.2794e-01,\n",
       "          -3.5771e-02,  8.2082e-02,  2.4744e-01, -2.3276e-02, -1.1531e-01,\n",
       "          -1.0086e-01, -2.6001e-01, -5.4109e-02, -1.0358e-01, -1.2263e-01,\n",
       "          -1.3541e-01,  6.9908e-02,  1.6285e-02, -5.1405e-03, -1.9429e-01,\n",
       "           4.9552e-02, -4.3809e-02, -1.1069e-01,  5.1855e-02,  1.5521e-04,\n",
       "           1.0733e-01, -7.6963e-02, -2.3123e-02,  1.5451e-01,  1.4658e-02,\n",
       "           1.1136e-01,  7.4246e-02, -5.8607e-02, -3.9791e-02, -1.6679e-01,\n",
       "           1.0684e-02, -5.9395e-02, -1.2557e-01, -1.7579e-01,  2.8731e-02,\n",
       "           2.8437e-02,  7.7271e-02,  5.7894e-02, -7.1932e-03, -1.5142e-01,\n",
       "           1.5143e-01, -2.0715e-01,  1.1441e-01], grad_fn=<SelectBackward>)),\n",
       " ('any',\n",
       "  tensor([ 3.0938e-01,  6.0800e-02,  2.2532e-01,  3.7465e-02,  1.1631e-01,\n",
       "           4.3430e-02, -7.8083e-02, -8.2572e-02, -1.2190e-01,  6.3920e-02,\n",
       "          -6.0641e-02,  8.2091e-02, -9.0306e-02,  1.9438e-01, -3.6102e-02,\n",
       "          -2.5222e-01, -5.4219e-02, -2.3182e-01,  2.4861e-02, -3.0264e-02,\n",
       "          -2.0414e-01, -8.4090e-02,  8.1236e-02, -3.3541e-02,  3.3644e-02,\n",
       "           2.0323e-01, -9.6498e-02,  1.4857e-01, -2.4096e-01,  1.4270e-01,\n",
       "           1.1771e-01, -3.4458e-02, -1.7750e-01,  5.0056e-02, -2.8043e-01,\n",
       "           1.7252e-01,  1.1813e-01,  1.7407e-01, -4.4237e-02,  1.6549e-01,\n",
       "          -8.2055e-02, -1.0021e-02, -4.8729e-02, -1.3537e-01, -1.1074e-01,\n",
       "           1.0612e-01, -3.4662e-01, -4.7428e-02, -2.6903e-02,  3.8707e-02,\n",
       "           2.7034e-02,  2.4015e-01, -2.0839e-03, -3.6776e-01,  2.5294e-02,\n",
       "          -1.5196e-01, -7.8058e-03, -7.5576e-02, -7.8066e-02,  1.6076e-01,\n",
       "          -9.0904e-02, -1.0940e-01, -1.6245e-01,  9.4563e-03, -1.1166e-01,\n",
       "           5.1385e-02, -4.2570e-02,  1.7983e-01, -6.1487e-02, -2.1784e-01,\n",
       "           1.0600e-01, -2.6607e-01, -1.4657e-02,  1.4292e-01, -1.3777e-01,\n",
       "          -1.2763e-02,  1.3451e-01, -1.1749e-01,  3.2823e-01,  8.6719e-02,\n",
       "           1.9754e-01,  2.1796e-01, -9.1786e-02,  1.8231e-01,  3.5800e-04,\n",
       "           1.1419e-01,  1.6575e-02,  2.1909e-01, -1.0758e-01, -2.3132e-01,\n",
       "          -7.7113e-02, -1.3010e-01, -9.3752e-02,  5.5670e-02, -1.8211e-01,\n",
       "          -8.9690e-02, -2.3114e-01, -5.9193e-02, -9.9836e-02, -1.1659e-01,\n",
       "           1.3985e-01, -1.8104e-01, -1.4200e-01,  7.7174e-02, -1.8619e-02,\n",
       "           7.3109e-02,  2.8271e-02, -1.0638e-01,  1.4702e-01, -1.3044e-01,\n",
       "          -1.3793e-01,  1.0039e-01, -1.7483e-02, -3.6504e-02,  1.2104e-02,\n",
       "           4.9303e-02, -2.8589e-01, -8.7550e-02, -2.8552e-01,  2.3732e-01,\n",
       "          -1.4368e-01,  8.2987e-02,  1.5996e-01,  8.2498e-02, -2.5905e-01,\n",
       "           9.3334e-02, -2.6624e-01,  7.8632e-02], grad_fn=<SelectBackward>))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1284"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.mapping_old_to_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import math\n",
    "# plot tree\n",
    "init_notebook_mode()\n",
    "def plot(G, annotations=False):\n",
    "\n",
    "    lay = G.layout_fruchterman_reingold(grid=\"nogrid\", niter=1000)\n",
    "    nr_vertices = len(G.vs)\n",
    "    position = {k: lay[k] for k in range(nr_vertices)}\n",
    "    Y = [lay[k][1] for k in range(nr_vertices)]\n",
    "    M = max(Y)\n",
    "    labels = [x['name'] for x in G.vs()]\n",
    "    es = igraph.EdgeSeq(G) # sequence of edges\n",
    "    E = [e.tuple for e in G.es] # list of edges\n",
    "\n",
    "    L = len(position)\n",
    "    Xn = [position[k][0] for k in range(L)]\n",
    "    Yn = [2*M-position[k][1] for k in range(L)]\n",
    "    Xe = []\n",
    "    Ye = []\n",
    "    for edge in E:\n",
    "        Xe+=[position[edge[0]][0],position[edge[1]][0], None]\n",
    "        Ye+=[2*M-position[edge[0]][1],2*M-position[edge[1]][1], None] \n",
    "\n",
    "\n",
    "    #Create Plotly Traces\n",
    "\n",
    "    lines = go.Scatter(x=Xe,\n",
    "                       y=Ye,\n",
    "                       mode='lines',\n",
    "                       line=dict(color='rgb(210,210,210)', width=1),\n",
    "                       hoverinfo='none'\n",
    "                       )\n",
    "    dots = go.Scatter(x=Xn,\n",
    "                      y=Yn,\n",
    "                      mode='markers',\n",
    "                      name='',\n",
    "                      marker=dict(  colorbar=dict(\n",
    "                                        title=\"Modularity\"\n",
    "                                    ),\n",
    "                                    colorscale=\"Viridis\",\n",
    "                                    \n",
    "                                  \n",
    "                                    line=dict(color='rgb(50,50,50)', width=1)\n",
    "                                    ),\n",
    "                      \n",
    "                      hoverinfo='text',\n",
    "                      opacity=0.8\n",
    "                      )\n",
    "\n",
    "    # Create Text Inside the Circle via Annotations\n",
    "\n",
    "    def make_annotations(pos, text, font_size=10, \n",
    "                         font_color='rgb(0,0,0)'):\n",
    "        L=len(pos)\n",
    "        if len(text)!=L:\n",
    "            raise ValueError('The lists pos and text must have the same len')\n",
    "        annotations = go.Annotations()\n",
    "        for k in range(L):\n",
    "            annotations.append(\n",
    "                go.Annotation(\n",
    "                    text=labels[k], # or replace labels with a different list \n",
    "                                    # for the text within the circle  \n",
    "                    x=pos[k][0], y=2*M-position[k][1],\n",
    "                    xref='x1', yref='y1',\n",
    "                    font=dict(color=font_color, size=font_size),\n",
    "                    showarrow=False)\n",
    "            )\n",
    "        return annotations  \n",
    "\n",
    "    # Add Axis Specifications and Create the Layout\n",
    "\n",
    "    axis = dict(showline=False, # hide axis line, grid, ticklabels and  title\n",
    "                zeroline=False,\n",
    "                showgrid=False,\n",
    "                showticklabels=False,\n",
    "                )\n",
    "    ann = []\n",
    "    if annotations:\n",
    "        ann = make_annotations(position, labels)\n",
    "    layout = dict(#title= 'Tree with Reingold-Tilford Layout',  \n",
    "                  height=900,\n",
    "                  annotations=ann,\n",
    "                  font=dict(size=12),\n",
    "                  showlegend=False,\n",
    "                  xaxis=go.XAxis(axis),\n",
    "                  yaxis=go.YAxis(axis),          \n",
    "                  margin=dict(l=40, r=40, b=85, t=100),\n",
    "                  hovermode='closest',\n",
    "                  plot_bgcolor='rgb(248,248,248)'          \n",
    "                  )\n",
    "\n",
    "    # Plot\n",
    "\n",
    "    data=go.Data([lines, dots])\n",
    "    fig=dict(data=data, layout=layout)\n",
    "    #fig['layout'].update(annotations=make_annotations(position, labels))\n",
    "    iplot(fig, filename='Tree-Reingold-Tilf')\n",
    "    # use py.plot instead of py.iplot if you're not using a Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,data,optimizer,epoch):\n",
    "        model.train()   \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        out = model.forward(data,data.adj_t)\n",
    "        samples = Sampler.sample(nodes)\n",
    "        loss = model.loss(out, samples)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optuna todo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "        data,mapping_new_to_old=data_loader()\n",
    "        print(data)\n",
    "        perm, ptr = metis(data.adj_t, num_parts=40, log=True)\n",
    "        data_train = permute(data, perm, log=True)\n",
    "        loader = SubgraphLoader(data_train, ptr, batch_size=10, shuffle=True)\n",
    "        len(loader)\n",
    "\n",
    "        hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "        out_layer = trial.suggest_categorical(\"out_layer\", [64,128])\n",
    "        size = trial.suggest_categorical(\"size of network, number of convs\", [2,3])\n",
    "        num_negative_samples=trial.suggest_categorical('num neg samples', [1,2,3,4,5,6,7,8,9,10])\n",
    "        model = Scalable(num_nodes=len(data.x), in_channels=len(data.x[0]), hidden_channels=hidden_layer,out_channels=out_layer, num_layers=size)\n",
    "        learning_rate= trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "\n",
    "        for epoch in range(50):\n",
    "            loss = train(model,loader,num_negative_smples)\n",
    "        score=test(y_true_val,model,data,val_edge_index)\n",
    "        \n",
    "        trial.report(score,epoch)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective,n_trials = 20)\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\" {}: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
